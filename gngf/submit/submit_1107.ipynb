{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# Author: Zhirui Zhou\n",
    "# Mail  : evilpsycho42@gmail.com\n",
    "# Time  : 11/5/18\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plant_power = {\n",
    "    1: 10,\n",
    "    2: 10,\n",
    "    3: 40,\n",
    "    4: 50\n",
    "}\n",
    "\n",
    "\n",
    "def mae_d(df_groupby, plant):\n",
    "    pm = df_groupby['pm'].values\n",
    "    pp = df_groupby['pp'].values\n",
    "    threshold = plant_power[plant] * 0.03\n",
    "    index = pm >= threshold\n",
    "    return np.abs(pm[index] - pp[index]).sum() / (np.sum(index) * plant_power[plant])\n",
    "\n",
    "\n",
    "def mae_m(df, plant):\n",
    "    return df.groupby(df['datetime'].dt.day).apply(lambda x: mae_d(x, plant)).mean()\n",
    "\n",
    "\n",
    "def score(df, plant):\n",
    "    \"\"\"\n",
    "\n",
    "    :param df: datetime, pm, pp\n",
    "    :param plant:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    month = df['datetime'].dt.month.unique()\n",
    "    ret = []\n",
    "    for m in month:\n",
    "        ret.append(mae_m(df[df['datetime'].dt.month == m], plant))\n",
    "    return np.mean(ret)\n",
    "\n",
    "\n",
    "def score2(pm, pp, plant):\n",
    "    threshold = plant_power[plant] * 0.03\n",
    "    index = pm >= threshold\n",
    "    return np.abs(pm[index] - pp[index]).sum() / (np.sum(index) * plant_power[plant])\n",
    "\n",
    "\n",
    "# coding:utf8\n",
    "# @Time    : 18-11-6 下午9:02\n",
    "# @Author  : evilpsycho\n",
    "# @Mail    : evilpsycho42@gmail.com\n",
    "import numpy as np\n",
    "from chinese_calendar import is_holiday\n",
    "\n",
    "\n",
    "def arithmetic_mapping(field_1, field_2, df):\n",
    "    ret = []\n",
    "    features = []\n",
    "    feature_cat = ['num'] * 4\n",
    "    for act in '+-*/':\n",
    "        ret.append(eval(f'df[field_1] {act} df[field_2]').values)\n",
    "        features.append(f'{field_1}{act}{field_2}')\n",
    "    ret = np.stack(ret, axis=1)\n",
    "    return ret, features, feature_cat\n",
    "\n",
    "\n",
    "def arithmetic_field_mapping(fields_1, fields_2, df):\n",
    "    field_combination = [(f1, f2) for f1 in fields_1 for f2 in fields_2]\n",
    "    ret, features, feature_cat = [], [], []\n",
    "    for f1, f2 in field_combination:\n",
    "        r, fs, fc = arithmetic_mapping(f1, f2, df)\n",
    "        ret.append(r)\n",
    "        features += fs\n",
    "        feature_cat += fc\n",
    "    return np.concatenate(ret, axis=1), features, feature_cat\n",
    "\n",
    "\n",
    "def origin_feature(df):\n",
    "    feature = df[['hm', 'irr', 'pr', 'temp', 'wd', 'ws']]\n",
    "    feature.columns = ['org '+col for col in feature.columns]\n",
    "    return feature.values, feature.columns.tolist(), ['num'] * 6\n",
    "\n",
    "def date_feature(df, time=True, hour=True, month=True,\n",
    "         weekday=True, holiday=True, year=True):\n",
    "    ret, features, feature_cat = [], [], []\n",
    "    if time:\n",
    "        ret.append((df.time.dt.minute + df.time.dt.hour * 60).values)\n",
    "        features.append('date_time')\n",
    "        feature_cat.append('num')\n",
    "    if hour:\n",
    "        ret.append(df.time.dt.hour)\n",
    "        features.append('date_hour')\n",
    "        feature_cat.append('num')\n",
    "    if month:\n",
    "        ret.append(df.time.dt.month)\n",
    "        features.append('date_month')\n",
    "        feature_cat.append('cat')\n",
    "    if weekday:\n",
    "        ret.append(df.time.dt.weekday)\n",
    "        features.append('date_weekday')\n",
    "        feature_cat.append('cat')\n",
    "    if year:\n",
    "        ret.append(df.time.dt.year)\n",
    "        features.append('date_year')\n",
    "        feature_cat.append('cat')\n",
    "    if holiday:\n",
    "        ret.append(df.time.apply(is_holiday).astype(np.int).values)\n",
    "        features.append('date_holiday')\n",
    "        feature_cat.append('cat')\n",
    "    if len(ret) == 0:\n",
    "        raise ValueError('必须输入至少一个特征')\n",
    "    return np.stack(ret, axis=1), features, feature_cat\n",
    "\n",
    "\n",
    "# coding:utf8\n",
    "# @Time    : 18-11-6 下午6:19\n",
    "# @Author  : evilpsycho\n",
    "# @Mail    : evilpsycho42@gmail.com\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _get_sample_weight(y, plant, w=5):\n",
    "    plant_power = {\n",
    "        1: 10,\n",
    "        2: 10,\n",
    "        3: 40,\n",
    "        4: 50\n",
    "    }\n",
    "    weights = np.ones_like(y)\n",
    "    weights[y > plant_power[plant] * 0.03] = w\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _score2(pm, pp, plant):\n",
    "    plant_power = {\n",
    "        1: 10,\n",
    "        2: 10,\n",
    "        3: 40,\n",
    "        4: 50\n",
    "    }\n",
    "    threshold = plant_power[plant] * 0.03\n",
    "    index = pm >= threshold\n",
    "    return np.abs(pm[index] - pp[index]).sum() / (np.sum(index) * plant_power[plant])\n",
    "\n",
    "\n",
    "def lgb_cv(params, x, y, plant, k=3, **kwargs):\n",
    "    kf = KFold(k, **kwargs)\n",
    "    weights = _get_sample_weight(y, plant)\n",
    "    ret = []\n",
    "\n",
    "    def metric(t, p):\n",
    "        return _score2(t, p, plant)\n",
    "\n",
    "    for train, valid in kf.split(x):\n",
    "        train_set = lgb.Dataset(x[train], y[train], weight=weights[train], **kwargs)\n",
    "        valid_set = lgb.Dataset(x[valid], y[valid], weight=weights[valid], **kwargs)\n",
    "        mdl = lgb.train(params, train_set, valid_sets=[train_set, valid_set], verbose_eval=-1)\n",
    "        ret.append(metric(y[valid], mdl.predict(x[valid])))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def lgb_grid_search_cv(param_grid, x, y, plant, k=5, **kwargs):\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    max_score = np.inf\n",
    "    best_param = None\n",
    "    n_step = len(grid)\n",
    "    for step, p in enumerate(grid):\n",
    "        score = np.mean(lgb_cv(p, x, y, plant=plant, k=k, **kwargs))\n",
    "        if score < max_score:\n",
    "            best_param = p\n",
    "            max_score = score\n",
    "            print(f'step {step / n_step * 100: .1f}%, best cv score: {max_score: .4f}')\n",
    "    return best_param, max_score\n",
    "\n",
    "\n",
    "def lgb_train(param, x, y, plant, **kwargs):\n",
    "    weights = _get_sample_weight(y, plant)\n",
    "    train_set = lgb.Dataset(x, y, weight=weights, **kwargs)\n",
    "    model = lgb.train(param, train_set, verbose_eval=10)\n",
    "    print(f'Plant {plant} trainset score: {_score2(y, model.predict(x), plant):.4f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def lgb_predict(model, x, idx):\n",
    "    y = model.predict(x)\n",
    "    pred = pd.DataFrame({\"id\": idx, \"predicition\": y})\n",
    "    pred['id'] = pred['id'].astype(int)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bagging_fraction': [0.7, 0.8], \n",
    "    'boosting': ['gbdt'], \n",
    "    'feature_fraction': [0.8, 0.9], \n",
    "    'learning_rate': [0.05, 0.1, 0.03] ,\n",
    "    'n_iter': range(75, 600, 50) ,\n",
    "    'num_leaves': [50, 31], \n",
    "    'objective': ['regression', 'regression_l1'],\n",
    "    'task': ['train']\n",
    "}\n",
    "\n",
    "param_feature_selector = {'bagging_fraction': 0.7, 'boosting': 'gbdt', 'feature_fraction': 0.8, \n",
    "                          'learning_rate': 0.1, 'n_iter': 100, 'num_leaves': 31, 'objective': 'regression_l1', \n",
    "                          'task': 'train'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading plant 1 data\n",
      "Plant 1 trainset score: 0.0895\n",
      "step  0.0%, best cv score:  0.1178\n",
      "step  0.2%, best cv score:  0.1159\n",
      "step  0.6%, best cv score:  0.1152\n",
      "step  18.8%, best cv score:  0.1151\n",
      "Plant 1 trainset score: 0.0946\n",
      "loading plant 2 data\n",
      "Plant 2 trainset score: 0.0956\n",
      "step  0.0%, best cv score:  0.1372\n",
      "step  0.2%, best cv score:  0.1329\n",
      "step  0.6%, best cv score:  0.1328\n",
      "step  1.3%, best cv score:  0.1322\n",
      "step  18.8%, best cv score:  0.1319\n",
      "step  19.5%, best cv score:  0.1318\n",
      "step  20.3%, best cv score:  0.1317\n",
      "Plant 2 trainset score: 0.0984\n",
      "loading plant 3 data\n",
      "Plant 3 trainset score: 0.0766\n",
      "step  0.0%, best cv score:  0.1315\n",
      "step  0.2%, best cv score:  0.1230\n",
      "step  0.6%, best cv score:  0.1214\n",
      "Plant 3 trainset score: 0.0858\n",
      "loading plant 4 data\n",
      "Plant 4 trainset score: 0.0819\n",
      "step  0.0%, best cv score:  0.1250\n",
      "step  0.2%, best cv score:  0.1235\n",
      "step  0.9%, best cv score:  0.1222\n",
      "step  8.9%, best cv score:  0.1220\n",
      "step  44.1%, best cv score:  0.1219\n",
      "Plant 4 trainset score: 0.0799\n"
     ]
    }
   ],
   "source": [
    "featureTopN = 40\n",
    "sampleWeight = 5\n",
    "\n",
    "\n",
    "plants = [1, 2, 3, 4]\n",
    "ret = []\n",
    "for plant in plants:\n",
    "    data = load_dataset(plant)\n",
    "    train = data.id.isnull()\n",
    "    test = data.power.isnull()\n",
    "\n",
    "    X = []\n",
    "    train_y = data[train].power.values\n",
    "    feature_name = []\n",
    "    feature_category = []\n",
    "\n",
    "    X_date, date_name, date_cate = date_feature(data)\n",
    "    X_org, org_name, org_cate = origin_feature(data)\n",
    "    X_map, map_name, map_cate = arithmetic_field_mapping(['hm', 'irr', 'pr', 'temp'], \n",
    "                                                         ['hm', 'irr', 'pr', 'temp'], data)\n",
    "\n",
    "    X = np.concatenate([X_date, X_org, X_map], axis=1)\n",
    "    feature_name = date_name + org_name + map_name\n",
    "    feature_category = date_cate + org_cate + map_cate\n",
    "\n",
    "    train_X = X[train]\n",
    "    test_X = X[test]\n",
    "    \n",
    "    feature_model = lgb_train(param_feature_selector, x=train_X, y=train_y, plant=plant)\n",
    "    feature_mask = np.where(np.argsort(-feature_model.feature_importance())<=featureTopN)[0]\n",
    "    best_param, best_score = lgb_grid_search_cv(param_grid=param_grid, \n",
    "                       x=train_X[: ,feature_mask],\n",
    "                       y=train_y,\n",
    "                       k=5, plant=plant)\n",
    "    print(best_param, best_score)\n",
    "    model = lgb_train(param=best_param, x=train_X[: ,feature_mask], y=train_y, plant=plant)\n",
    "    ret_plant = lgb_predict(model, x=test_X[: ,feature_mask], idx=data[test]['id'].values)\n",
    "    ret.append(ret_plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ret = pd.concat(ret)\n",
    "ret.to_csv(\"/home/zhouzr/桌面/submit_20181107v2.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
