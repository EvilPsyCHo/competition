{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chinese_calendar import is_holiday\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "plant_power = {\n",
    "    1: 10,\n",
    "    2: 10,\n",
    "    3: 40,\n",
    "    4: 50\n",
    "}\n",
    "\n",
    "\n",
    "def score2(pm, pp, plant):\n",
    "    threshold = plant_power[plant] * 0.03\n",
    "    index = pm >= threshold\n",
    "    return np.abs(pm[index] - pp[index]).sum() / (np.sum(index) * plant_power[plant])\n",
    "\n",
    "\n",
    "\n",
    "def _get_sample_weight(y, plant, w=5):\n",
    "    plant_power = {\n",
    "        1: 10,\n",
    "        2: 10,\n",
    "        3: 40,\n",
    "        4: 50\n",
    "    }\n",
    "    weights = np.ones_like(y)\n",
    "    weights[y > plant_power[plant] * 0.03] = w\n",
    "    return weights\n",
    "\n",
    "\n",
    "def _score2(pm, pp, plant):\n",
    "    plant_power = {\n",
    "        1: 10,\n",
    "        2: 10,\n",
    "        3: 40,\n",
    "        4: 50\n",
    "    }\n",
    "    threshold = plant_power[plant] * 0.03\n",
    "    index = pm >= threshold\n",
    "    return np.abs(pm[index] - pp[index]).sum() / (np.sum(index) * plant_power[plant])\n",
    "\n",
    "\n",
    "def lgb_cv(params, x, y, plant, shuffle, k=5, weight=5, **kwargs):\n",
    "    kf = KFold(k, shuffle=shuffle, **kwargs)\n",
    "    weights = _get_sample_weight(y, plant, w=weight)\n",
    "    ret = []\n",
    "\n",
    "    def metric(t, p):\n",
    "        return _score2(t, p, plant)\n",
    "\n",
    "    for train, valid in kf.split(x):\n",
    "        train_set = lgb.Dataset(x[train], y[train], weight=weights[train], **kwargs)\n",
    "        valid_set = lgb.Dataset(x[valid], y[valid], weight=weights[valid], **kwargs)\n",
    "        mdl = lgb.train(params, train_set, valid_sets=[train_set, valid_set], verbose_eval=-1)\n",
    "        ret.append(metric(y[valid], mdl.predict(x[valid])))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def lgb_grid_search_cv(param_grid, x, y, plant, shuffle, k=5, **kwargs):\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    max_score = np.inf\n",
    "    best_param = None\n",
    "    n_step = len(grid)\n",
    "    for step, p in enumerate(grid):\n",
    "        score = np.mean(lgb_cv(p, x, y, shuffle, plant=plant, k=k, **kwargs))\n",
    "        if score < max_score:\n",
    "            best_param = p\n",
    "            max_score = score\n",
    "            print(f'step {step / n_step * 100: .1f}%, best cv score: {max_score: .4f}')\n",
    "    return best_param, max_score\n",
    "\n",
    "\n",
    "\n",
    "def lgb_train(param, x, y, plant, **kwargs):\n",
    "    weights = _get_sample_weight(y, plant)\n",
    "    train_set = lgb.Dataset(x, y, weight=weights, **kwargs)\n",
    "    model = lgb.train(param, train_set, verbose_eval=10)\n",
    "    print(f'Plant {plant} trainset score: {_score2(y, model.predict(x), plant):.4f}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def lgb_predict(model, x, idx):\n",
    "    y = model.predict(x)\n",
    "    pred = pd.DataFrame({\"id\": idx, \"predicition\": y})\n",
    "    pred['id'] = pred['id'].astype(int)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(plant):\n",
    "    train = pd.read_csv(f'../data/train_{plant}.csv', parse_dates=[\"时间\"]).drop_duplicates().reset_index(drop=True)\n",
    "    test = pd.read_csv(f'../data/test_{plant}.csv', parse_dates=[\"时间\"])\n",
    "    train.columns = ['time', 'irr', 'ws', 'wd', 'temp', 'pr', 'hm', 'mirr', 'power']\n",
    "    test.columns = ['id', 'time', 'irr', 'ws', 'wd', 'temp', 'pr', 'hm']\n",
    "    data = pd.concat([train, test])\n",
    "    return data\n",
    "\n",
    "def data_preprocessing(data):\n",
    "    print(f'data preprocessing ...')\n",
    "    data['time'] = data['time'] + dt.timedelta(seconds=1.)\n",
    "    # 处理异常power\n",
    "    # 处理异常其他值\n",
    "    return data\n",
    "\n",
    "\n",
    "def date_feature(df, night=False, hour=True, month=True, month_hour=True,\n",
    "         weekday=False, holiday=False, year=False):\n",
    "    ret, features, feature_cat = [], [], []\n",
    "    night_idx = ((df.time.dt.hour <= 5) | (df.time.dt.hour >= 20))\n",
    "    if night:\n",
    "        ret.append(night_idx.astype(int))\n",
    "        features.append('date_night')\n",
    "        feature_cat.append('cat')\n",
    "    if hour:\n",
    "        hour = df.time.dt.hour + df.time.dt.minute / 60\n",
    "        hour = hour.apply(lambda x: '0' if x <= 5 or x >=20 else f'{x}')\n",
    "        hour = LabelEncoder().fit_transform(hour)\n",
    "        ret.append(hour)\n",
    "        features.append('date_hour')\n",
    "        feature_cat.append('cat')\n",
    "    if month:\n",
    "        ret.append(df.time.dt.month)\n",
    "        features.append('date_month')\n",
    "        feature_cat.append('cat')\n",
    "    if month_hour:\n",
    "        hour_map = lambda x: '0' if x <= 5 or x >=20 else f'{x}'\n",
    "        hour = df.time.dt.hour.apply(hour_map)\n",
    "        month = df.time.dt.month.astype(str)\n",
    "        hour_month = LabelEncoder().fit_transform(hour + month)\n",
    "        ret.append(hour_month)\n",
    "        features.append('date_hour_month')\n",
    "        feature_cat.append('cat')\n",
    "    if weekday:\n",
    "        ret.append(df.time.dt.weekday)\n",
    "        features.append('date_weekday')\n",
    "        feature_cat.append('cat')\n",
    "    if year:\n",
    "        ret.append(df.time.dt.year)\n",
    "        features.append('date_year')\n",
    "        feature_cat.append('cat')\n",
    "    if holiday:\n",
    "        ret.append(df.time.apply(is_holiday).astype(np.int).values)\n",
    "        features.append('date_holiday')\n",
    "        feature_cat.append('cat')\n",
    "    if len(ret) == 0:\n",
    "        raise ValueError('必须输入至少一个特征')\n",
    "    return np.stack(ret, axis=1), features, feature_cat\n",
    "\n",
    "\n",
    "def cross_irr_month(df):\n",
    "    # month & irr_lvl\n",
    "    irr_lvl = pd.cut(df['irr'], 20, labels=[f'irr_{i}' for i in range(20)])\n",
    "    month = df.time.dt.month.astype(str)\n",
    "    irr_month = LabelEncoder().fit_transform(irr_lvl + month)\n",
    "    return irr_month.reshape(-1, 1), ['cross_irr_month'], ['cat']\n",
    "\n",
    "\n",
    "def arithmetic_mapping(field_1, field_2, df):\n",
    "    ret = []\n",
    "    features = []\n",
    "    feature_cat = ['num'] * 4\n",
    "    for act in '+-*/':\n",
    "        ret.append(eval(f'df[field_1] {act} df[field_2]').values)\n",
    "        features.append(f'{field_1}{act}{field_2}')\n",
    "    ret = np.stack(ret, axis=1)\n",
    "    return ret, features, feature_cat\n",
    "\n",
    "\n",
    "def arithmetic_field_mapping(fields_1, fields_2, df):\n",
    "    field_combination = [(f1, f2) for f1 in fields_1 for f2 in fields_2]\n",
    "    ret, features, feature_cat = [], [], []\n",
    "    for f1, f2 in field_combination:\n",
    "        r, fs, fc = arithmetic_mapping(f1, f2, df)\n",
    "        ret.append(r)\n",
    "        features += fs\n",
    "        feature_cat += fc\n",
    "    return np.concatenate(ret, axis=1), features, feature_cat\n",
    "\n",
    "\n",
    "def origin_feature(df):\n",
    "    feature = df[['hm', 'irr', 'pr', 'temp', 'wd', 'ws']]\n",
    "    feature.columns = ['org '+col for col in feature.columns]\n",
    "    return feature.values, feature.columns.tolist(), ['num'] * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzr/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant 1 trainset score: 0.0915\n",
      "date_hour_month    261\n",
      "date_hour          233\n",
      "date_month         193\n",
      "org irr            140\n",
      "org wd             118\n",
      "org ws             113\n",
      "irr-temp           111\n",
      "pr+temp             99\n",
      "hm+temp             97\n",
      "irr*irr             91\n",
      "org hm              86\n",
      "cross_irr_month     70\n",
      "irr/temp            59\n",
      "irr+temp            58\n",
      "temp-irr            58\n",
      "hm-pr               57\n",
      "pr*temp             53\n",
      "hm/pr               52\n",
      "hm*temp             48\n",
      "hm/temp             48\n",
      "hm*pr               44\n",
      "org temp            44\n",
      "pr-temp             43\n",
      "temp/pr             41\n",
      "irr*temp            40\n",
      "pr-hm               38\n",
      "pr/temp             38\n",
      "irr+pr              33\n",
      "temp-hm             32\n",
      "hm-temp             32\n",
      "irr+irr             31\n",
      "irr-hm              30\n",
      "temp*temp           30\n",
      "temp+hm             30\n",
      "temp/irr            28\n",
      "hm+pr               28\n",
      "temp-pr             28\n",
      "hm+irr              26\n",
      "hm-irr              25\n",
      "temp+pr             22\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lgb_cv() got multiple values for argument 'plant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-a7374a77ae7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                    \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                    \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                    k=5, plant=plant)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-642e1e5ee2af>\u001b[0m in \u001b[0;36mlgb_grid_search_cv\u001b[0;34m(param_grid, x, y, plant, shuffle, k, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mn_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgb_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: lgb_cv() got multiple values for argument 'plant'"
     ]
    }
   ],
   "source": [
    "featureTopN = 40\n",
    "sampleWeight = 5\n",
    "\n",
    "param_grid_full = {\n",
    "    'boosting': ['gbdt'], \n",
    "    'learning_rate': [0.05, 0.03, 0.01] ,\n",
    "    'num_iterations': range(500, 1000, 50) ,\n",
    "    'num_leaves': [50, 60, 70],\n",
    "    'max_depth': [7],\n",
    "    'bagging_fraction': [0.7, 0.9],\n",
    "    'feature_fraction': [0.7, 0.9],\n",
    "    'objective': ['regression'],\n",
    "    'task': ['train']\n",
    "}\n",
    "\n",
    "param_feature_selector = {'bagging_fraction': 0.7, 'boosting': 'gbdt', 'feature_fraction': 0.8, \n",
    "                          'learning_rate': 0.1, 'n_iter': 100, 'num_leaves': 31, 'objective': 'regression_l1', \n",
    "                          'task': 'train'}\n",
    "\n",
    "plant = 1\n",
    "choice_n = 5000\n",
    "\n",
    "data = load_dataset(plant)\n",
    "train = data.id.isnull()\n",
    "test = data.power.isnull()\n",
    "\n",
    "X = []\n",
    "train_y = data[train].power.values\n",
    "feature_name = []\n",
    "feature_category = []\n",
    "\n",
    "X_date, date_name, date_cate = date_feature(data)\n",
    "X_org, org_name, org_cate = origin_feature(data)\n",
    "X_cross1, cross1_name, cross1_cate = cross_irr_month(data)\n",
    "X_map, map_name, map_cate = arithmetic_field_mapping(['hm', 'irr', 'pr', 'temp'], \n",
    "                                                     ['hm', 'irr', 'pr', 'temp'], data)\n",
    "\n",
    "X = np.concatenate([X_date, X_org, X_map, X_cross1], axis=1)\n",
    "feature_name = date_name + org_name + map_name + cross1_name\n",
    "feature_category = date_cate + org_cate + map_cate + cross1_cate\n",
    "\n",
    "train_X = X[train]\n",
    "test_X = X[test]\n",
    "\n",
    "choice = np.random.choice(train_X.shape[0], size=choice_n, replace=False)\n",
    "\n",
    "feature_model = lgb_train(param_feature_selector, x=train_X, y=train_y, plant=plant)\n",
    "feature_mask = np.where(np.argsort(-feature_model.feature_importance())<=featureTopN)[0]\n",
    "feature_importance = \\\n",
    "pd.Series(feature_model.feature_importance(), index=feature_name).sort_values(ascending=False)\n",
    "print(feature_importance[: featureTopN])\n",
    "best_param, best_score = lgb_grid_search_cv(param_grid=param_grid_full, \n",
    "                   x=train_X[choice, :][:, feature_mask],\n",
    "                   y=train_y[choice], shuffle=True,\n",
    "                   k=5, plant=plant)\n",
    "print(best_param, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzr/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [20, 4]\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [20, 9]\n",
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [20, 10]\n",
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [20, 11]\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [25, 7]\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [40, 5]\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "0.118954573838 [40, 7]\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n",
      "Plant 1 trainset score: 0.0912\n"
     ]
    }
   ],
   "source": [
    "featureTopN = 40\n",
    "sampleWeight = 5\n",
    "\n",
    "max_score = 10\n",
    "best = None\n",
    "\n",
    "for f in range(20,45,5):\n",
    "    for w in range(4,12,1):\n",
    "        feature_model = lgb_train(param_feature_selector, x=train_X, y=train_y, plant=plant)\n",
    "        feature_mask = np.where(np.argsort(-feature_model.feature_importance())<=featureTopN)[0]\n",
    "        s = lgb_cv(best_param, train_X[:, feature_mask], train_y, k=5, plant=1, weight=w)\n",
    "        s = np.mean(s)\n",
    "        if s <= max_score:\n",
    "            best = [f, w]\n",
    "            max_score = s\n",
    "            print(max_score, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzr/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant 1 trainset score: 0.0900\n",
      "step  0.0%, best cv score:  0.1178\n",
      "step  0.2%, best cv score:  0.1152\n",
      "step  0.6%, best cv score:  0.1152\n",
      "step  0.9%, best cv score:  0.1151\n",
      "step  1.3%, best cv score:  0.1149\n",
      "step  2.1%, best cv score:  0.1149\n",
      "step  18.0%, best cv score:  0.1148\n",
      "step  18.8%, best cv score:  0.1146\n",
      "step  19.5%, best cv score:  0.1145\n",
      "{'bagging_fraction': 0.7, 'boosting': 'gbdt', 'feature_fraction': 0.8, 'learning_rate': 0.03, 'n_iter': 225, 'num_leaves': 31, 'objective': 'regression_l1', 'task': 'train'} 0.114459876371\n",
      "Plant 1 trainset score: 0.0931\n",
      "Plant 2 trainset score: 0.0947\n",
      "step  0.0%, best cv score:  0.1360\n",
      "step  0.2%, best cv score:  0.1339\n",
      "step  1.3%, best cv score:  0.1338\n",
      "step  2.1%, best cv score:  0.1336\n",
      "step  18.0%, best cv score:  0.1332\n",
      "step  18.8%, best cv score:  0.1328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-c0ba411c0547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                        \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mfeature_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                        \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                        k=5, plant=plant)\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mfeature_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-444a8af703e3>\u001b[0m in \u001b[0;36mlgb_grid_search_cv\u001b[0;34m(param_grid, x, y, plant, k, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mn_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgb_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-444a8af703e3>\u001b[0m in \u001b[0;36mlgb_cv\u001b[0;34m(params, x, y, plant, k, weight, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "featureTopN = 40\n",
    "sampleWeight = 5\n",
    "\n",
    "param_grid = {\n",
    "    'bagging_fraction': [0.7, 0.8], \n",
    "    'boosting': ['gbdt'], \n",
    "    'feature_fraction': [0.8, 0.9], \n",
    "    'learning_rate': [0.05, 0.1, 0.03] ,\n",
    "    'n_iter': range(75, 600, 50) ,\n",
    "    'num_leaves': [50, 31], \n",
    "    'objective': ['regression', 'regression_l1'],\n",
    "    'task': ['train']\n",
    "}\n",
    "\n",
    "param_feature_selector = {'bagging_fraction': 0.7, 'boosting': 'gbdt', 'feature_fraction': 0.8, \n",
    "                          'learning_rate': 0.1, 'n_iter': 100, 'num_leaves': 31, 'objective': 'regression_l1', \n",
    "                          'task': 'train'}\n",
    "\n",
    "\n",
    "plants = [1, 2, 3, 4]\n",
    "ret = []\n",
    "for plant in plants:\n",
    "    data = load_dataset(plant)\n",
    "    train = data.id.isnull()\n",
    "    test = data.power.isnull()\n",
    "\n",
    "    X = []\n",
    "    train_y = data[train].power.values\n",
    "    feature_name = []\n",
    "    feature_category = []\n",
    "\n",
    "    X_date, date_name, date_cate = date_feature(data)\n",
    "    X_org, org_name, org_cate = origin_feature(data)\n",
    "    X_cross1, cross1_name, cross1_cate = cross_irr_month(data)\n",
    "    X_map, map_name, map_cate = arithmetic_field_mapping(['hm', 'irr', 'pr', 'temp', 'wd', 'ws'], \n",
    "                                                         ['hm', 'irr', 'pr', 'temp', 'wd', 'ws'], data)\n",
    "\n",
    "    X = np.concatenate([X_date, X_org, X_map, X_cross1], axis=1)\n",
    "    feature_name = date_name + org_name + map_name + cross1_name\n",
    "    feature_category = date_cate + org_cate + map_cate + cross1_cate\n",
    "\n",
    "    train_X = X[train]\n",
    "    test_X = X[test]\n",
    "    \n",
    "    feature_model = lgb_train(param_feature_selector, x=train_X, y=train_y, plant=plant)\n",
    "    feature_mask = np.where(np.argsort(-feature_model.feature_importance())<=featureTopN)[0]\n",
    "    best_param, best_score = lgb_grid_search_cv(param_grid=param_grid, \n",
    "                       x=train_X[: ,feature_mask],\n",
    "                       y=train_y,\n",
    "                       k=5, plant=plant)\n",
    "    print(best_param, best_score)\n",
    "    model = lgb_train(param=best_param, x=train_X[: ,feature_mask], y=train_y, plant=plant)\n",
    "    ret_plant = lgb_predict(model, x=test_X[: ,feature_mask], idx=data[test]['id'].values)\n",
    "    ret.append(ret_plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzr/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plant 1 trainset score: 0.0900\n",
      "0.117100306285\n",
      "Plant 1 trainset score: 0.1016\n",
      "Plant 2 trainset score: 0.0947\n",
      "0.133799048207\n",
      "Plant 2 trainset score: 0.1092\n",
      "Plant 3 trainset score: 0.0742\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-ae5d7eacec52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#                        k=5, plant=plant)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#     print(best_param, best_score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mfeature_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-444a8af703e3>\u001b[0m in \u001b[0;36mlgb_cv\u001b[0;34m(params, x, y, plant, k, weight, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p = {\n",
    "    'bagging_fraction': 0.9, 'boosting': 'gbdt', \n",
    "    'feature_fraction': 1.0, \n",
    "    'learning_rate': 0.05, \n",
    "    'n_iter': 200, \n",
    "    'num_leaves': 20, \n",
    "    'max_depth': 6,\n",
    "    'objective': 'regression_l1', \n",
    "    'task': 'train'\n",
    "}\n",
    "\n",
    "plants = [1, 2, 3, 4]\n",
    "ret = []\n",
    "for plant in plants:\n",
    "    data = load_dataset(plant)\n",
    "    train = data.id.isnull()\n",
    "    test = data.power.isnull()\n",
    "\n",
    "    X = []\n",
    "    train_y = data[train].power.values\n",
    "    feature_name = []\n",
    "    feature_category = []\n",
    "\n",
    "    X_date, date_name, date_cate = date_feature(data)\n",
    "    X_org, org_name, org_cate = origin_feature(data)\n",
    "    X_cross1, cross1_name, cross1_cate = cross_irr_month(data)\n",
    "    X_map, map_name, map_cate = arithmetic_field_mapping(['hm', 'irr', 'pr', 'temp', 'wd', 'ws'], \n",
    "                                                         ['hm', 'irr', 'pr', 'temp', 'wd', 'ws'], data)\n",
    "\n",
    "    X = np.concatenate([X_date, X_org, X_map, X_cross1], axis=1)\n",
    "    feature_name = date_name + org_name + map_name + cross1_name\n",
    "    feature_category = date_cate + org_cate + map_cate + cross1_cate\n",
    "\n",
    "    train_X = X[train]\n",
    "    test_X = X[test]\n",
    "    \n",
    "    feature_model = lgb_train(param_feature_selector, x=train_X, y=train_y, plant=plant)\n",
    "    feature_mask = np.where(np.argsort(-feature_model.feature_importance())<=20)[0]\n",
    "#     best_param, best_score = lgb_grid_search_cv(param_grid=param_grid, \n",
    "#                        x=train_X[: ,feature_mask],\n",
    "#                        y=train_y,\n",
    "#                        k=5, plant=plant)\n",
    "#     print(best_param, best_score)\n",
    "    s = lgb_cv(p, train_X[:, feature_mask], train_y, k=5, plant=plant)\n",
    "    print(np.mean(s))\n",
    "    model = lgb_train(param=p, x=train_X[: ,feature_mask], y=train_y, plant=plant)\n",
    "    ret_plant = lgb_predict(model, x=test_X[: ,feature_mask], idx=data[test]['id'].values)\n",
    "    ret.append(ret_plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = pd.concat(ret)\n",
    "ret.to_csv(\"/home/zhouzr/桌面/submit_20181109v2.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
